{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import KFold\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import cPickle\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = '../'  # '/media/xiaoxy/2018-Kaggle-AdTrackingFraud/'\n",
    "predictors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################### Helper function ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_onehot(df, column_name):\n",
    "    df_onehot = pd.get_dummies(df[column_name], prefix=column_name)\n",
    "    df_all = pd.concat([df.drop([column_name], axis=1), df_onehot], axis=1)\n",
    "    predictors.append(column_name)\n",
    "    return df_all\n",
    "\n",
    "\n",
    "def encode_count(df, column_name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(list(df[column_name].values))\n",
    "    df[column_name] = le.transform(list(df[column_name].values))\n",
    "    predictors.append(column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_count(df, columns_groupby, new_column_name, type='uint64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby).size()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_nunique(df, columns_groupby, column, new_column_name, type='uint64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].nunique()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_cumcount(df, columns_groupby, column, new_column_name, type='uint64'):\n",
    "    df[new_column_name] = df.groupby(columns_groupby)[column].cumcount().values.astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_median(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].median()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_mean(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].mean()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_sum(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].sum()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    # predictors.append(new_column_name)  # bug: twice\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_max(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].max()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_min(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].min()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_std(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].std()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_var(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].var()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_rank(df, columns_groupby, column, new_column_name, ascending=True, type='uint64'):\n",
    "    df[new_column_name] = df.groupby(columns_groupby)[column].rank(ascending=ascending)\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_count(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feat.groupby(columns_groupby)[column].count()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_count.columns = columns_groupby + [column + \"_gb_%s_count\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_count.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_count, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_count.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_nunique(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_nunique = pd.DataFrame(df_feat.groupby(columns_groupby)[column].nunique()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_nunique.columns = columns_groupby + [column + \"_%s_nunique\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_nunique.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_nunique, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_nunique.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_mean(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_mean = pd.DataFrame(df_feat.groupby(columns_groupby)[column].mean()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_mean.columns = columns_groupby + [column + \"_%s_mean\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_mean.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_mean, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_mean.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_std(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_std = pd.DataFrame(df_feat.groupby(columns_groupby)[column].std()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_std.columns = columns_groupby + [column + \"_%s_std\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_std.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_std, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_std.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_median(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_median = pd.DataFrame(df_feat.groupby(columns_groupby)[column].median()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_median.columns = columns_groupby + [column + \"_%s_median\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_median.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_median, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_median.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_max(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_max = pd.DataFrame(df_feat.groupby(columns_groupby)[column].max()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_max.columns = columns_groupby + [column + \"_%s_max\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_max.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_max, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_max.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_min(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_min = pd.DataFrame(df_feat.groupby(columns_groupby)[column].min()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_min.columns = columns_groupby + [column + \"_%s_min\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_min.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_min, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_min.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_sum(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_sum = pd.DataFrame(df_feat.groupby(columns_groupby)[column].sum()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_sum.columns = columns_groupby + [column + \"_%s_sum\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_sum.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_sum, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_sum.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_var(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_var = pd.DataFrame(df_feat.groupby(columns_groupby)[column].var()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_var.columns = columns_groupby + [column + \"_%s_var\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_var.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_var, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_var.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_quantile(df, df_feat, columns_groupby, column, quantile_n, new_column_name=\"\"):\n",
    "    df_quantile = pd.DataFrame(df_feat.groupby(columns_groupby)[column].quantile(quantile_n)).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_quantile.columns = columns_groupby + [column + \"_%s_quantile\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_quantile.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_quantile, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_quantile.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_feat_skew(df, df_feat, columns_groupby, column, new_column_name=\"\"):\n",
    "    df_skew = pd.DataFrame(df_feat.groupby(columns_groupby)[column].skew()).reset_index()\n",
    "    if not new_column_name:\n",
    "        df_skew.columns = columns_groupby + [column + \"_%s_skew\" % (\"_\".join(columns_groupby))]\n",
    "    else:\n",
    "        df_skew.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(df_skew, on=columns_groupby, how=\"left\").fillna(0)\n",
    "    predictors.append(df_skew.columns[-1])\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_rank_sp(df, feat1, feat2, ascending):\n",
    "    df.sort_values([feat1, feat2], inplace=True, ascending=ascending)\n",
    "    df['rank'] = range(df.shape[0])\n",
    "    min_rank = df.groupby(feat1, as_index=False)['rank'].agg({'min_rank': 'min'})\n",
    "    df = pd.merge(df, min_rank, on=feat1, how='left')\n",
    "    df['rank'] = df['rank'] - df['min_rank']\n",
    "    predictors.append('rank')\n",
    "    del df['min_rank']\n",
    "    return df\n",
    "\n",
    "\n",
    "def log(info):\n",
    "    print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' ' + str(info)\n",
    "\n",
    "\n",
    "def log_shape(train, test):\n",
    "    log('Train data shape: %s' % str(train.shape))\n",
    "    log('Test data shape: %s' % str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_date(df):\n",
    "    format = '%Y-%m-%d %H:%M:%S'\n",
    "    df['date'] = pd.to_datetime(df['click_time'], format=format)\n",
    "    df['month'] = df['date'].dt.month.astype('uint8')\n",
    "    df['weekday'] = df['date'].dt.weekday.astype('uint8')\n",
    "    df['day'] = df['date'].dt.day.astype('uint8')\n",
    "    df['hour'] = df['date'].dt.hour.astype('uint8')\n",
    "    df['minute'] = df['date'].dt.minute.astype('uint8')\n",
    "    df['second'] = df['date'].dt.second.astype('uint8')\n",
    "    df['tm_hour'] = (df['hour'] + df['minute'] / 60.0).astype('float32')\n",
    "    df['tm_hour_sin'] = (df['tm_hour'].map(lambda x: math.sin((x - 12) / 24 * 2 * math.pi))).astype('float32')\n",
    "    df['tm_hour_cos'] = (df['tm_hour'].map(lambda x: math.cos((x - 12) / 24 * 2 * math.pi))).astype('float32')\n",
    "    del df['click_time']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### Construct features function - begin ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_next_time_delta(df, suffix, type='float32'):\n",
    "    groupby_columns = [\n",
    "        {'columns': ['ip', 'app', 'channel', 'device', 'os']},\n",
    "        {'columns': ['ip', 'os', 'device']},\n",
    "        {'columns': ['ip', 'os', 'device', 'app']}\n",
    "    ]\n",
    "    # Calculate the time to next click for each group\n",
    "    for spec in groupby_columns:\n",
    "        # Name of new feature\n",
    "        new_name = '{}_{}'.format('_'.join(spec['columns']), suffix)\n",
    "        # Unique list of features to select\n",
    "        all_features = spec['columns'] + ['date']\n",
    "        # Run calculation\n",
    "        log('Calculate ' + suffix + '...')\n",
    "        df[new_name] = (df[all_features].groupby(spec['columns']).date.shift(-1) - df.date).dt.seconds.astype(type)\n",
    "        predictors.append(new_name)\n",
    "        gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def cal_prev_time_delta(df, suffix, type='float32'):\n",
    "    groupby_columns = [\n",
    "        {'columns': ['ip', 'channel']},\n",
    "        {'columns': ['ip', 'os']}\n",
    "    ]\n",
    "    # Calculate the time to prev click for each group\n",
    "    for spec in groupby_columns:\n",
    "        # Name of new feature\n",
    "        new_name = '{}_{}'.format('_'.join(spec['columns']), suffix)\n",
    "        # Unique list of features to select\n",
    "        all_features = spec['columns'] + ['date']\n",
    "        # Run calculation\n",
    "        log('Calculate ' + suffix + '...')\n",
    "        df[new_name] = (df.date - df[all_features].groupby(spec['columns']).date.shift(+1)).dt.seconds.astype(type)\n",
    "        predictors.append(new_name)\n",
    "        gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def cal_cvr(train, test, type='float32'):\n",
    "    train['cvr_gb_ip_day_hour'] = 0\n",
    "    train['cvr_gb_ip_app'] = 0\n",
    "    train['cvr_gb_ip_app_os'] = 0\n",
    "\n",
    "    # Define group by list\n",
    "    idh = ['ip', 'day', 'hour']\n",
    "    ia = ['ip', 'app']\n",
    "    iao = ['ip', 'app', 'os']\n",
    "\n",
    "    kf = KFold(train.shape[0], n_folds=5, shuffle=True, random_state=7)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        log('Fold ' + str(i) + ' begin...')\n",
    "\n",
    "        # Divide train/test fold\n",
    "        tr = train.iloc[train_index, :train.shape[1] - 3]\n",
    "        te = train.iloc[test_index, :train.shape[1] - 3]\n",
    "\n",
    "        # Calculate sum of label of train folds\n",
    "        log('Cal sum_label_gb_ip_day_hour')\n",
    "        tr = merge_sum(tr, idh, 'is_attributed', 'sum_label_gb_ip_day_hour')\n",
    "        log('Cal sum_label_gb_ip_app')\n",
    "        tr = merge_sum(tr, ia, 'is_attributed', 'sum_label_gb_ip_app')\n",
    "        log('Cal sum_label_gb_ip_app_os')\n",
    "        tr = merge_sum(tr, iao, 'is_attributed', 'sum_label_gb_ip_app_os')\n",
    "\n",
    "        # Calculate cvr of train folds with using smothing technique\n",
    "        tr['cvr_gb_ip_day_hour'] = GaussianSmoth().update_moment(tr['count_gb_ip_day_hour'], tr['sum_label_gb_ip_day_hour'])\n",
    "        tr['cvr_gb_ip_app'] = GaussianSmoth().update_moment(tr['count_gb_ip_app'], tr['sum_label_gb_ip_app'])\n",
    "        tr['cvr_gb_ip_app_os'] = GaussianSmoth().update_moment(tr['count_gb_ip_app_os'], tr['sum_label_gb_ip_app_os'])\n",
    "\n",
    "        # Merge test fold with cvr features of train folds\n",
    "        te = te.merge(tr[['cvr_gb_ip_day_hour'] + idh].drop_duplicates(subset=idh, keep='first'), on=idh, how='left')\n",
    "        te = te.merge(tr[['cvr_gb_ip_app'] + ia].drop_duplicates(subset=ia, keep='first'), on=ia, how='left')\n",
    "        te = te.merge(tr[['cvr_gb_ip_app_os'] + iao].drop_duplicates(subset=iao, keep='first'), on=iao, how='left')\n",
    "\n",
    "        # Put it in train\n",
    "        train['cvr_gb_ip_day_hour'] += te['cvr_gb_ip_day_hour']\n",
    "        train['cvr_gb_ip_app'] += te['cvr_gb_ip_app']\n",
    "        train['cvr_gb_ip_app_os'] += te['cvr_gb_ip_app_os']\n",
    "\n",
    "        del tr, te\n",
    "        log('Fold ' + str(i) + ' Done!')\n",
    "\n",
    "    # Convert type\n",
    "    train['cvr_gb_ip_day_hour'] = train['cvr_gb_ip_day_hour'].astype(type)\n",
    "    train['cvr_gb_ip_app'] = train['cvr_gb_ip_app'].astype(type)\n",
    "    train['cvr_gb_ip_app_os'] = train['cvr_gb_ip_app_os'].astype(type)\n",
    "\n",
    "    # Merge cvr of train to test\n",
    "    test = test.merge(train[['cvr_gb_ip_day_hour'] + idh].drop_duplicates(subset=idh, keep='first'), on=idh, how='left')\n",
    "    test = test.merge(train[['cvr_gb_ip_app'] + ia].drop_duplicates(subset=ia, keep='first'), on=ia, how='left')\n",
    "    test = test.merge(train[['cvr_gb_ip_app_os'] + iao].drop_duplicates(subset=iao, keep='first'), on=iao, how='left')\n",
    "\n",
    "    predictors.append('cvr_gb_ip_day_hour')\n",
    "    predictors.append('cvr_gb_ip_app')\n",
    "    predictors.append('cvr_gb_ip_app_os')\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### Construct features function - end ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spilt_local_train_test(df, train_size, test_size):\n",
    "    local_train = df[:train_size]\n",
    "    local_test = df[train_size:train_size + test_size]\n",
    "    return local_train, local_test\n",
    "\n",
    "\n",
    "def get_model_input_data(train, test, is_local):\n",
    "    feat = ['ip', 'app', 'device', 'os', 'channel', 'hour']\n",
    "    for f in feat:\n",
    "        if f not in predictors:\n",
    "            predictors.append(f)\n",
    "    train_x = train[predictors]\n",
    "    train_y = train.is_attributed.values\n",
    "    if is_local == 1:\n",
    "        test_x = test[train_x.columns.values]\n",
    "        test_y = test.is_attributed.values\n",
    "        return train_x, train_y, test_x, test_y\n",
    "    else:\n",
    "        test_x = test[train_x.columns.values]\n",
    "        return train_x, train_y, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_cv(train_feature, train_label, test_feature, test_label, params, folds, rounds):\n",
    "    start = time.clock()\n",
    "    print train_feature.columns\n",
    "    params['scale_pos_weights'] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    dtest = xgb.DMatrix(test_feature, label=test_label)\n",
    "    num_round = rounds\n",
    "    watchlist = [(dtrain, 'train'), (dtest, 'test')]\n",
    "    print('XGBoost run cv: ' + 'round: ' + str(rounds))\n",
    "    res = xgb.train(params, dtrain, num_round, watchlist, verbose_eval=20, early_stopping_rounds=50)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print('Time used:' + str(elapsed) + 's')\n",
    "    return res.best_ntree_limit, res.best_score, res\n",
    "\n",
    "\n",
    "def xgb_predict(train_feature, train_label, test_feature, rounds, params):\n",
    "    params['scale_pos_weights'] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    dtest = xgb.DMatrix(test_feature, label=np.zeros(test_feature.shape[0]))\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    num_round = rounds\n",
    "    model = xgb.train(params, dtrain, num_round, watchlist, verbose_eval=30)\n",
    "    predict = model.predict(dtest)\n",
    "    return model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgb_cv(train_feature, train_label, test_feature, test_label, params, folds, rounds):\n",
    "    start = time.clock()\n",
    "    print(train_feature.columns)\n",
    "    params['scale_pos_weight'] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = lgb.Dataset(train_feature, label=train_label, categorical_feature=['app', 'device', 'os', 'channel', 'hour'])\n",
    "    dtest = lgb.Dataset(test_feature, label=test_label, categorical_feature=['app', 'device', 'os', 'channel', 'hour'])\n",
    "    num_round = rounds\n",
    "    print('LightGBM run cv: ' + 'round: ' + str(rounds))\n",
    "    res = lgb.train(params, dtrain, num_round, valid_sets=[dtest], valid_names=['test'], verbose_eval=1, early_stopping_rounds=20)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print('Time used:', elapsed, 's')\n",
    "    return res.best_iteration, res.best_score['test']['auc'], res\n",
    "\n",
    "\n",
    "def lgb_predict(train_feature, train_label, test_feature, rounds, params):\n",
    "    dtrain = lgb.Dataset(train_feature, label=train_label, categorical_feature=['app', 'device', 'os', 'channel', 'hour'])\n",
    "    num_round = rounds\n",
    "    model = lgb.train(params, dtrain, num_round, valid_sets=[dtrain], verbose_eval=1)\n",
    "    predict = model.predict(test_feature)\n",
    "    return model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_result(test_index, pred, name):\n",
    "    result = pd.DataFrame({'click_id': test_index, 'is_attributed': pred})\n",
    "    result.to_csv(root_path + 'data/output/sub/' + name + '.csv', index=False, sep=',')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GaussianSmoth(object):\n",
    "    def __init__(self, alpha=0, beta=0):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_moment(self, tries, success):\n",
    "        '''estimate alpha, beta using moment estimation'''\n",
    "        mean, var = self.__compute_moment(tries, success)\n",
    "        self.alpha = (mean + 0.000001) * ((mean + 0.000001) * (1.000001 - mean) / (var + 0.000001) - 1)\n",
    "        self.beta = (1.000001 - mean) * ((mean + 0.000001) * (1.000001 - mean) / (var + 0.000001) - 1)\n",
    "        print self.alpha, self.beta\n",
    "        return (self.alpha + success) / (self.alpha + self.beta + tries)\n",
    "\n",
    "    def __compute_moment(self, tries, success):\n",
    "        # Cal mean and variance\n",
    "        '''moment estimation'''\n",
    "        ctr_list = []\n",
    "        mean = (success / tries).mean()\n",
    "        if len(tries) == 1:\n",
    "            var = 0\n",
    "        else:\n",
    "            var = (success / tries).var()\n",
    "        return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################### Read data ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Read data...')\n",
    "dtypes = {\n",
    "    'click_id': 'uint32',\n",
    "    'ip': 'uint32',\n",
    "    'app': 'uint16',\n",
    "    'device': 'uint16',\n",
    "    'os': 'uint16',\n",
    "    'channel': 'uint16',\n",
    "    'is_attributed': 'uint8'\n",
    "}\n",
    "train = pd.read_csv(root_path + 'data/input/train.csv', header=0, sep=',', dtype=dtypes, usecols=['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed'])\n",
    "test_supplement = pd.read_csv(root_path + 'data/input/test_supplement.csv', header=0, sep=',', dtype=dtypes, usecols=['ip', 'app', 'device', 'os', 'channel', 'click_time'])\n",
    "gc.collect()\n",
    "log('Read data done!')\n",
    "log_shape(train, test_supplement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################### Preprocess ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Process date...')\n",
    "train = process_date(train)\n",
    "test_supplement = process_date(test_supplement)\n",
    "gc.collect()\n",
    "log('Process date done!')\n",
    "log_shape(train, test_supplement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################### Feature engineer ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_len = len(train)\n",
    "log('Train size:' + str(train_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Train append test_supplement...')\n",
    "df = train.append(test_supplement).reset_index(drop=True)\n",
    "del train\n",
    "del test_supplement\n",
    "gc.collect()\n",
    "log('Train append test_supplement done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Before feature engineer')\n",
    "log('Num of features: ' + str(len(df.columns)))\n",
    "log('Features: ' + str(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Cal next_time_delta')\n",
    "df = cal_next_time_delta(df, 'next_time_delta', 'float32')\n",
    "gc.collect()\n",
    "log('Cal prev_time_delta')\n",
    "df = cal_prev_time_delta(df, 'prev_time_delta', 'float32')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log('Cal nunique_channel_gb_ip')\n",
    "df = merge_nunique(df, ['ip'], 'channel', 'nunique_channel_gb_ip', 'uint32')\n",
    "gc.collect()\n",
    "log('Cal nunique_app_gb_ip_device_os')\n",
    "df = merge_nunique(df, ['ip', 'device', 'os'], 'app', 'nunique_app_gb_ip_device_os', 'uint32')\n",
    "gc.collect()\n",
    "log('Cal nunique_hour_gb_ip_day')\n",
    "df = merge_nunique(df, ['ip', 'day'], 'hour', 'nunique_hour_gb_ip_day', 'uint32')\n",
    "gc.collect()\n",
    "log('Cal nunique_app_gb_ip')\n",
    "df = merge_nunique(df, ['ip'], 'app', 'nunique_app_gb_ip', 'uint32')\n",
    "gc.collect()\n",
    "log('Cal nunique_os_gb_ip_app')\n",
    "df = merge_nunique(df, ['ip', 'app'], 'os', 'nunique_os_gb_ip_app', 'uint32')\n",
    "gc.collect()\n",
    "log('Cal nunique_device_gb_ip')\n",
    "df = merge_nunique(df, ['ip'], 'device', 'nunique_device_gb_ip', 'uint32')\n",
    "gc.collect()\n",
    "log('Cal nunique_channel_gb_app')\n",
    "df = merge_nunique(df, ['app'], 'channel', 'nunique_channel_gb_app', 'uint32')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Cal cumcount_os_gb_ip')\n",
    "df = merge_cumcount(df, ['ip'], 'os', 'cumcount_os_gb_ip', 'uint32');\n",
    "gc.collect()\n",
    "log('Cal cumcount_app_gb_ip_device_os')\n",
    "df = merge_cumcount(df, ['ip', 'device', 'os'], 'app', 'cumcount_app_gb_ip_device_os', 'uint32');\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Cal count_gb_ip_day_hour')\n",
    "df = merge_count(df, ['ip', 'day', 'hour'], 'count_gb_ip_day_hour', 'uint32');\n",
    "gc.collect()\n",
    "log('Cal count_gb_ip_app')\n",
    "df = merge_count(df, ['ip', 'app'], 'count_gb_ip_app', 'uint32');\n",
    "gc.collect()\n",
    "log('Cal count_gb_ip_app_os')\n",
    "df = merge_count(df, ['ip', 'app', 'os'], 'count_gb_ip_app_os', 'uint32');\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Cal var_day_gb_ip_app_os')\n",
    "df = merge_var(df, ['ip', 'app', 'os'], 'day', 'var_day_gb_ip_app_os', 'float32')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct features done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('After feature engineer')\n",
    "log('Num of features: ' + str(len(df.columns)))\n",
    "log('Features: ' + str(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### All features save & reload - begin ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save all features\n",
    "cPickle.dump(df, open(root_path + 'data/output/feat/all.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Reload all features\n",
    "# df = cPickle.load(open(root_path + 'data/output/feat/all.p', 'rb'))\n",
    "# train_len = 184903891\n",
    "# dtypes = {\n",
    "#     'click_id': 'uint32',\n",
    "#     'ip': 'uint32',\n",
    "#     'app': 'uint16',\n",
    "#     'device': 'uint16',\n",
    "#     'os': 'uint16',\n",
    "#     'channel': 'uint16',\n",
    "#     'is_attributed': 'uint8'\n",
    "# }\n",
    "# predictors = ['ip', 'app', 'device', 'os', 'channel', 'hour',\n",
    "#               'next_time_delta', 'prev_time_delta',\n",
    "#               'nunique_channel_gb_ip', 'nunique_app_gb_ip_device_os',\n",
    "#               'nunique_hour_gb_ip_day', 'nunique_app_gb_ip', 'nunique_os_gb_ip_app',\n",
    "#               'nunique_device_gb_ip', 'nunique_channel_gb_app',\n",
    "#               'cumcount_os_gb_ip', 'cumcount_app_gb_ip_device_os',\n",
    "#               'count_gb_ip_day_hour', 'count_gb_ip_app', 'count_gb_ip_app_os',\n",
    "#               'var_day_gb_ip_app_os']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### All features save & reload - end ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Train test_supplement divid...')\n",
    "train = df[:train_len]\n",
    "test_supplement = df[train_len:]\n",
    "del df\n",
    "gc.collect()\n",
    "log_shape(train, test_supplement)\n",
    "log('Train test_supplement divid done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Read test...')\n",
    "test = pd.read_csv(root_path + 'data/input/test.csv', header=0, sep=',', dtype=dtypes, usecols=['click_id', 'ip', 'app', 'device', 'os', 'channel', 'click_time'], parse_dates=['click_time'])\n",
    "log('Test data original shape: ' + str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.merge(test_supplement.drop_duplicates(subset=['ip', 'app', 'device', 'os', 'channel', 'date'], keep='first'), left_on=['ip', 'app', 'device', 'os', 'channel', 'click_time'], right_on=['ip', 'app', 'device', 'os', 'channel', 'date'], how='left')\n",
    "test.drop(['click_time'], axis=1, inplace=True)\n",
    "del test_supplement\n",
    "gc.collect()\n",
    "log_shape(train, test)\n",
    "log('Read test done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cal cvr features\n",
    "log('Cal cvr...')\n",
    "train, test = cal_cvr(train, test, 'float32')\n",
    "log('Cal cvr done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### CVR features save & reload - begin ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvr_feats = ['cvr_gb_ip_day_hour', 'cvr_gb_ip_app', 'cvr_gb_ip_app_os']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save cvr features\n",
    "cPickle.dump(train[cvr_feats], open(root_path + 'data/output/feat/train_cvr.p', 'wb'))\n",
    "cPickle.dump(test[cvr_feats], open(root_path + 'data/output/feat/test_cvr.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Reload cvr features\n",
    "# train_cvr = cPickle.load(open(root_path + 'data/output/feat/train_cvr.p', 'rb'))\n",
    "# test_cvr = cPickle.load(open(root_path + 'data/output/feat/test_cvr.p', 'rb'))\n",
    "# train = pd.concat([train, train_cvr], axis=1)\n",
    "# test_cvr = pd.concat([test, test_cvr], axis=1)\n",
    "# del train_cvr, test_cvr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### CVR features save & reload - end ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################### Split dataset for local ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Split dataset to get local train/test set...')\n",
    "local_train_size = 10000000  # 182403890\n",
    "local_test_size = 2500000\n",
    "local_train, local_test = spilt_local_train_test(train, local_train_size, local_test_size)\n",
    "log('Split dataset to get local train/test set done!')\n",
    "\n",
    "log('================================= Local data info =====================================')\n",
    "log('Local train shape:' + str(local_train.shape))\n",
    "log('Local test shape:' + str(local_test.shape))\n",
    "log('Local train label ratio (0-1):' + str(local_train.is_attributed.value_counts().values * 1.0 / local_train.shape[0]))\n",
    "log('Local train label number (0-1):' + str(local_train.is_attributed.value_counts().values))\n",
    "log('Local train min/max date:' + str(local_train.date.min()) + ',' + str(local_train.date.max()))\n",
    "log('Local test min/max date:' + str(local_test.date.min()) + ',' + str(local_test.date.max()))\n",
    "log('=======================================================================================')\n",
    "\n",
    "log('================================= Online data info =====================================')\n",
    "log('Online train shape:' + str(train.shape))\n",
    "log('Online test shape:' + str(test.shape))\n",
    "log('Online train label ratio (0-1):' + str(train.is_attributed.value_counts().values * 1.0 / train.shape[0]))\n",
    "log('Online train label number (0-1):' + str(train.is_attributed.value_counts().values))\n",
    "log('Online train min/max date:' + str(train.date.min()) + ',' + str(train.date.max()))\n",
    "log('Online train min/max date:' + str(test.date.min()) + ',' + str(test.date.max()))\n",
    "log('=======================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Get local model input data...')\n",
    "local_train_x, local_train_y, local_test_x, local_test_y = get_model_input_data(local_train, local_test, is_local=1)\n",
    "del local_train\n",
    "del local_test\n",
    "gc.collect()\n",
    "log_shape(local_train_x, local_test_x)\n",
    "log('Get local model input data done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log('Get online model input data...')\n",
    "online_train_x, online_train_y, online_test_x = get_model_input_data(train, test, is_local=0)\n",
    "del train\n",
    "del test\n",
    "gc.collect()\n",
    "log_shape(online_train_x, online_test_x)\n",
    "log('Get online model input data done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################### Model ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################### XGBoost ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_xgb = {\n",
    "    'rounds': 10000,\n",
    "    'folds': 5\n",
    "}\n",
    "\n",
    "params_xgb = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'stratified': True,\n",
    "    'scale_pos_weights': 51,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 2,\n",
    "    'gamma': 1,\n",
    "    'subsample': 0.75,\n",
    "    'colsample_bytree': 0.75,\n",
    "    'lambda': 1,\n",
    "\n",
    "    'eta': 0.01,\n",
    "    'seed': 20,\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations_xgb, best_score_xgb = xgb_cv(local_train_x, local_train_y, local_test_x, local_test_y, params_xgb, config_xgb['folds'], config_xgb['rounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_xgb, pred_xgb = xgb_predict(online_train_x, online_train_y, online_test_x, iterations_xgb, params_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance_xgb = pd.DataFrame(model_xgb.get_fscore().items(), columns=['feature', 'importance']).sort_values('importance', ascending=False)\n",
    "importance_xgb.to_csv(root_path + 'data/output/feat_imp/importance-xgb-20180425-%f(r%d).csv' % (best_score_xgb, iterations_xgb), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_xgb = store_result(test.click_id, pred_xgb, '20180425-xgb-%f(r%d)' % (best_score_xgb, iterations_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################### LigthGBM ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_lgb = {\n",
    "    'rounds': 10000,\n",
    "    'folds': 5\n",
    "}\n",
    "\n",
    "params_lgb = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'xentropy',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.02,\n",
    "    # 'is_unbalance': 'true',  # Because training data is unbalance (replaced with scale_pos_weight)\n",
    "    'scale_pos_weight': 200,  # Because training data is extremely unbalanced\n",
    "    'num_leaves': 31,  # We should let it be smaller than 2^(max_depth)\n",
    "    'max_depth': -1,  # -1 means no limit\n",
    "    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 128,  # Number of bucketed bin for feature values\n",
    "    'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # Frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "    'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "    'reg_alpha': 0.99,  # L1 regularization term on weights\n",
    "    'reg_lambda': 0.9,  # L2 regularization term on weights\n",
    "    'nthread': 24,\n",
    "    'verbose': 1,\n",
    "    'seed': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations_lgb, best_score_lgb, model_cv_lgb = lgb_cv(local_train_x, local_train_y, local_test_x, local_test_y, params_lgb, config_lgb['folds'], config_lgb['rounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_lgb = model_cv_lgb.predict(online_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lgb, pred_lgb = lgb_predict(online_train_x, online_train_y, online_test_x, iterations_lgb, params_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance_lgb = sorted(zip(online_train_x.columns, model_cv_lgb.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True)\n",
    "importance_lgb = pd.DataFrame({'feature': importance_lgb})\n",
    "importance_lgb = importance_lgb.apply(lambda x: pd.Series(x['feature']), axis=1)\n",
    "importance_lgb.columns = ['feature', 'importance']\n",
    "importance_lgb.to_csv(root_path + 'data/output/feat_imp/importance-lgb-20180507-%f(r%d).csv' % (best_score_lgb, iterations_lgb), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_lgb = store_result(pd.read_csv(root_path + 'data/input/test.csv', header=0, sep=',', usecols=['click_id']).click_id.astype(int), pred_lgb, '20180507-lgb-%f(r%d)' % (best_score_lgb, iterations_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### Model save and reload - begin ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "log('Save model...')\n",
    "model_lgb.save_model(root_path + 'data/output/model/lgb-%f(r%d).txt' % (best_score_lgb, iterations_lgb))\n",
    "log('Model best score:' + str(best_score_lgb))\n",
    "log('Model best iteration:' + str(iterations_lgb))\n",
    "log('Save model done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Reload model\n",
    "# model_lgb = lgb.Booster(model_file=root_path + 'data/output/model/lgb-0.981609(r2100).txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### Model save and reload - end ###########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
